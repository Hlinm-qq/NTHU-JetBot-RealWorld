{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collision Avoidance - Live Demo\n",
    "\n",
    "In this notebook we'll use the model we trained to detect whether the robot is ``free`` or ``blocked`` to enable a collision avoidance behavior on the robot.  \n",
    "\n",
    "\n",
    "#### [realImg ~> unet ~> robot control]\n",
    "\n",
    "## Load the trained model\n",
    "\n",
    "We'll assumed that you've already downloaded the ``best_model.pth`` to your workstation as instructed in the training notebook.  Now, you should upload this model into this notebook's\n",
    "directory by using the Jupyter Lab upload tool.  Once that's finished there should be a file named ``best_model.pth`` in this notebook's directory.  \n",
    "\n",
    "> Please make sure the file has uploaded fully before calling the next cell\n",
    "\n",
    "Execute the code below to initialize the PyTorch model.  This should look very familiar from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torchvision\n",
    "\n",
    "# model = torchvision.models.alexnet(pretrained=False)\n",
    "# model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained weights from the ``best_model.pth`` file that you uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda')\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the preprocessing function\n",
    "\n",
    "We have now loaded our model, but there's a slight issue.  The format that we trained our model doesnt *exactly* match the format of the camera.  To do that, \n",
    "we need to do some *preprocessing*.  This involves the following steps\n",
    "\n",
    "1. Convert from BGR to RGB\n",
    "2. Convert from HWC layout to CHW layout\n",
    "3. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "4. Transfer the data from CPU memory to GPU memory\n",
    "5. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import signal\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from unet.unet import UNet\n",
    "# from jetbotSim import Robot, Env\n",
    "# from rl import dqn, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(state, img_size, th=0.4):\n",
    "    # x = state\n",
    "    # state = cv2.cvtColor(state, cv2.COLOR_BGR2RGB)\n",
    "    state = np.array(Image.fromarray(state).resize(img_size, Image.BILINEAR))\n",
    "    state = state.astype(float) / 255.\n",
    "    state = state.transpose(2,0,1)\n",
    "    return state\n",
    "\n",
    "def step(action):\n",
    "    global robot, dist\n",
    "    if action == 0:\n",
    "        robot.set_motors(0.03, 0.03)\n",
    "    elif action == 1:\n",
    "        robot.set_motors(0.2, 0.)\n",
    "    elif action == 2:\n",
    "        robot.set_motors(0., 0.2)\n",
    "    elif action == 3:\n",
    "        robot.stop()\n",
    "    elif action == 4:\n",
    "        robot.reset()\n",
    "    elif action == 99:\n",
    "        Kp = 0.1\n",
    "        left_wheel = 0.18\n",
    "        right_wheel = 0.17\n",
    "        forward_Kp = 0.7\n",
    "        range_ = 5\n",
    "        if dist > 100:\n",
    "            dist = 80\n",
    "        elif abs(dist) < 10:\n",
    "            dist *= 1.1\n",
    "\n",
    "#         print (dist)\n",
    "        if dist < -range_:  # right\n",
    "            # robot.set_motor(0.2, 0.2 - (blocked_right/100 + dist/1000))\n",
    "#             robot.right(Kp * abs(dist))\n",
    "            robot.set_motors(left_wheel*(Kp * abs(dist)), 0.05*(Kp * abs(dist)))\n",
    "            print(\"right\")\n",
    "        elif dist > range_: # left\n",
    "            # robot.set_motor(0.2 - 0.2*(blocked_right/100 + dist/1000), 0.2)\n",
    "#             robot.left(Kp * abs(dist))\n",
    "            robot.set_motors(0.05*(Kp * abs(dist)), right_wheel*(Kp * abs(dist)))\n",
    "            print(\"left\")\n",
    "        else:\n",
    "            robot.set_motors(left_wheel*forward_Kp, right_wheel*forward_Kp)\n",
    "            print(\"straight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_img(full_img, out_threshold=0.5):\n",
    "    global transform, device, net\n",
    "    r, g, b = full_img.split()\n",
    "    img = Image.merge(\"RGB\", (b, g, r))\n",
    "    img = transform(img)\n",
    "    img = img.unsqueeze(0).to(device=device)\n",
    "    with torch.no_grad():\n",
    "        output = net(img).cpu()\n",
    "        output = F.interpolate(output, (full_img.size[1], full_img.size[0]), mode='bilinear')\n",
    "        if net.n_classes > 1:\n",
    "            mask = output.argmax(dim=1)\n",
    "        else:\n",
    "            mask = torch.sigmoid(output) > out_threshold\n",
    "\n",
    "    return mask[0].long().squeeze().numpy()\n",
    "\n",
    "def process_unet_img(image_bgr):\n",
    "    global dist, img_size, ISPATH\n",
    "    # Convert BGR to RGB\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    # Create a PIL Image object from the RGB image\n",
    "    ori_img = Image.fromarray(image_rgb)\n",
    "\n",
    "    mask = predict_img(ori_img)\n",
    "    # classes = mask.max() + 1\n",
    "\n",
    "    path_mask = np.array(mask==1)\n",
    "\n",
    "    # Initialize the mask array with all white pixels and set black pixels where values are True\n",
    "    path_mask = np.where(path_mask[:, :, None], [0, 0, 0], [255, 255, 255]).astype(np.uint8)\n",
    "\n",
    "    img = cv2.resize(path_mask, img_size)\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "#     print(img.shape)\n",
    "\n",
    "    lower_black = np.array([0, 0, 0])\n",
    "    upper_black = np.array([10, 10, 10])\n",
    "\n",
    "    mask_road = cv2.inRange(hsv, lower_black, upper_black)\n",
    "    fin_img = cv2.bitwise_and(img, img, mask=mask_road)\n",
    "\n",
    "    # calculate next position\n",
    "    h_Kp1 = 0.65\n",
    "    h_Kp2 = 0.7\n",
    "    v_Kp = 0.1\n",
    "#     coord1 = cv2.findNonZero(mask_road[int(img_size[0]*h_Kp1):int(img_size[0]*h_Kp1) + 1, int(img_size[1]*v_Kp):int(img_size[1]-img_size[1]*v_Kp)])\n",
    "#     coord2 = cv2.findNonZero(mask_road[int(img_size[0]*h_Kp2):int(img_size[0]*h_Kp2) + 1, int(img_size[1]*v_Kp):int(img_size[1]-img_size[1]*v_Kp)])\n",
    "    coord1 = cv2.findNonZero(mask_road[int(img_size[0]*h_Kp1):int(img_size[0]*h_Kp1) + 1])\n",
    "    coord2 = cv2.findNonZero(mask_road[int(img_size[0]*h_Kp2):int(img_size[0]*h_Kp2) + 1])\n",
    "\n",
    "    if coord1 is not None and coord2 is not None:\n",
    "        left1 = np.min(coord1, axis=0)\n",
    "        right1 = np.max(coord1, axis=0)\n",
    "        left2 = np.min(coord2, axis=0)\n",
    "        right2 = np.max(coord2, axis=0)\n",
    "        # print(coord.shape)\n",
    "        # print(coord[:5, :])\n",
    "        line_mean1 = img_size[0] / 2\n",
    "        line_mean1 = int(np.mean([left1[0][0], right1[0][0]]))\n",
    "        line_mean2 = img_size[0] / 2\n",
    "        line_mean2 = int(np.mean([left2[0][0], right2[0][0]]))\n",
    "\n",
    "        line_mean3 = int(np.mean([line_mean1, line_mean2]))\n",
    "        \n",
    "        # print(left1, right1)\n",
    "        # print(left2, right2)\n",
    "        dist = img_size[0] / 2 - line_mean3\n",
    "        ISPATH = True\n",
    "    else:\n",
    "        dist = img_size[0] / 2\n",
    "        ISPATH = False\n",
    "    \n",
    "    print(\"coords\", dist)\n",
    "\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "# stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "# normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "# def preprocess(camera_value):\n",
    "#     global device, normalize\n",
    "#     x = camera_value\n",
    "#     x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "#     x = x.transpose((2, 0, 1))\n",
    "#     x = torch.from_numpy(x).float()\n",
    "#     x = normalize(x)\n",
    "#     x = x.to(device)\n",
    "#     x = x[None, ...]\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "qnet_model_dir = \"./rl_checkpoint/\"\n",
    "qnet_model_name = \"qnet_200000_jetbot.pt\"\n",
    "# model_path = \"./unet/checkpoints/checkpoint_epoch4_jetbot.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet_model_path = \"./unet/checkpoints2/checkpoint_epoch5_64_jetbot.pth\"\n",
    "\n",
    "\n",
    "frames = 0\n",
    "total_reward = 0\n",
    "stack_frames = 3\n",
    "img_size = (64, 64)\n",
    "\n",
    "recent_distances = []  # List to store recent distance measurements\n",
    "MAX_HISTORY = 4\n",
    "dist = 0\n",
    "ISPATH = False\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_all(model_dir, model_name, unet_model_path):\n",
    "  global total_reward, stack_frames, img_size, device\n",
    "  print(f\"Device: {device}\")\n",
    "\n",
    "  # Load UNET\n",
    "  net = UNet(n_channels=3, n_classes=3, bilinear=False)\n",
    "  net.to(device=device)\n",
    "  state_dict = torch.load(unet_model_path, map_location=device)\n",
    "  mask_values = state_dict.pop('mask_values', [0, 1, 2])\n",
    "  net.load_state_dict(state_dict)\n",
    "  net.eval()\n",
    "\n",
    "  # Load QNET\n",
    "  # agent = dqn.DeepQNetwork(\n",
    "  #     n_actions = 3,\n",
    "  #     input_shape = [(stack_frames)*3, *img_size],\n",
    "  #     qnet = models.QNet,\n",
    "  #     device = device,\n",
    "  #     learning_rate = 2e-4, \n",
    "  #     reward_decay = 0.98,\n",
    "  #     replace_target_iter = 1000, \n",
    "  #     memory_size = 10000,\n",
    "  #     batch_size = 32,\n",
    "  # )\n",
    "  # agent.save_load_model(\"load\", path=model_dir, fname=model_name)\n",
    "  \n",
    "  # return net, agent\n",
    "  return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net = None\n",
    "\n",
    "net = initialize_all(qnet_model_dir, qnet_model_name, unet_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes.  This function will do the following steps\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. While the neural network output indicates we're blocked, we'll turn left, otherwise we go forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "Now, let's start and display our camera.  You should be pretty familiar with this by now.  We'll also create a slider that will display the\n",
    "probability that the robot is blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3107a85e34048039098c71145fa9674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([image, blocked_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'update' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7dcc3e505cfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'new'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# we call the function once to intialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'update' is not defined"
     ]
    }
   ],
   "source": [
    "update({'new': camera.value})  # we call the function once to intialize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing. \n",
    "\n",
    "We accomplish that with the ``observe`` function.\n",
    "\n",
    "> WARNING: This code will move the robot!! Please make sure your robot has clearance.  The collision avoidance should work, but the neural\n",
    "> network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2494: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords -1.0\n",
      "straight\n",
      "coords -1.0\n",
      "straight\n",
      "coords -12.0\n",
      "right\n",
      "coords -19.0\n",
      "right\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 6.0\n",
      "left\n",
      "coords 6.0\n",
      "left\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 6.0\n",
      "left\n",
      "coords 6.0\n",
      "left\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 4.0\n",
      "straight\n",
      "coords 5.0\n",
      "left\n",
      "coords 5.0\n",
      "left\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 4.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 3.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 0.0\n",
      "straight\n",
      "coords 0.0\n",
      "straight\n",
      "coords -1.0\n",
      "straight\n",
      "coords -1.0\n",
      "straight\n",
      "coords -7.0\n",
      "right\n",
      "coords -8.0\n",
      "right\n",
      "coords -4.0\n",
      "straight\n",
      "coords -4.0\n",
      "straight\n",
      "coords -14.0\n",
      "right\n",
      "coords -15.0\n",
      "right\n",
      "coords -1.0\n",
      "straight\n",
      "coords -2.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 2.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 1.0\n",
      "straight\n",
      "coords 0.0\n",
      "straight\n",
      "coords 0.0\n",
      "straight\n",
      "coords -6.0\n",
      "right\n",
      "coords -6.0\n",
      "right\n",
      "coords -8.0\n",
      "right\n",
      "coords -9.0\n",
      "right\n",
      "coords -1.0\n",
      "straight\n",
      "coords -1.0\n",
      "straight\n",
      "coords -2.0\n",
      "straight\n",
      "coords -2.0\n",
      "straight\n",
      "coords -3.0\n",
      "straight\n",
      "coords -3.0\n",
      "straight\n",
      "coords -5.0\n",
      "right\n",
      "coords -5.0\n",
      "right\n",
      "coords -3.0\n",
      "straight\n",
      "coords -3.0\n",
      "straight\n",
      "coords -5.0\n",
      "right\n",
      "coords -4.0\n",
      "straight\n"
     ]
    }
   ],
   "source": [
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame.  Perhaps start by placing your robot on the ground and seeing what it does when it reaches an obstacle.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "camera.unobserve(update, names='value')\n",
    "\n",
    "time.sleep(0.1)  # add a small sleep to make sure frames have finished processing\n",
    "\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want the robot to run without streaming video to the browser.  You can unlink the camera as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.unlink()  # don't stream to browser (will still run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue streaming call the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_link.link()  # stream to browser (wont run camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "That's it for this live demo!  Hopefully you had some fun and your robot avoided collisions intelligently! \n",
    "\n",
    "If your robot wasn't avoiding collisions very well, try to spot where it fails.  The beauty is that we can collect more data for these failure scenarios\n",
    "and the robot should get even better :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
